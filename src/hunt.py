#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Used-Car Scoutï¼ˆDiscordé€šçŸ¥ç‰ˆ / ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ + åˆ†ä½å›å¸°ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼‰
- JSãªã—ã§å–å¾—ã§ãã‚‹ç¯„å›²ã®ãƒªã‚¹ãƒˆ/ãƒªãƒ³ã‚¯éƒ¨ã‹ã‚‰ å¹´å¼/è·é›¢/ä¾¡æ ¼/ã‚¿ã‚¤ãƒˆãƒ«æ–‡å­— ã‚’æŠ½å‡º
- ç›¸å ´1: åŒå›åãƒ‡ãƒ¼ã‚¿ã®ä¾¡æ ¼ä¸­å¤®å€¤ã‹ã‚‰ price_ratio ã‚’ç®—å‡ºï¼ˆå¸¸æ™‚ï¼‰
- ç›¸å ´2: åé›†ä»¶æ•°ãŒååˆ†ãªå›ã¯ã€åˆ†ä½å›å¸°(Quantile GB)ã®OOFã§ p50(ä¸­å¤®å€¤) / p20(ä¸‹ä½20%) ã‚’æ¨å®š
    * ãŠå¾—åº¦ = p50 - å®Ÿå£²ä¾¡æ ¼ ã§ã‚¹ã‚³ã‚¢/ç·Šæ€¥åº¦ã‚’å¾ŒæŠ¼ã—
    * å®Ÿå£²ä¾¡æ ¼ â‰¤ p20 ãªã‚‰å¼·ã„å‰²å®‰ã¨ã—ã¦ã•ã‚‰ã«ãƒ–ãƒ¼ã‚¹ãƒˆ
- ç·Šæ€¥åº¦4ä»¥ä¸Šã ã‘ Discord ã«é€šçŸ¥ã€‚ä¸Šä½ã‚’ results.csv ã«ä¿å­˜
ç’°å¢ƒå¤‰æ•°:
  DISCORD_WEBHOOK_URL  â€¦ Discord Incoming Webhook URLï¼ˆå¿…é ˆ/é€šçŸ¥ï¼‰
  TARGETS_JSON         â€¦ ç›£è¦–ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä¸Šæ›¸ãï¼ˆä»»æ„ã®JSONæ–‡å­—åˆ—ï¼‰
"""
from __future__ import annotations
import os, re, csv, json, time, math
from datetime import datetime
from typing import List, Dict, Any

import requests
from bs4 import BeautifulSoup

import numpy as np
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import KFold

UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/124.0 Safari/537.36"
)
HEADERS = {"User-Agent": UA, "Accept-Language": "ja,en;q=0.9"}

# --- ç›£è¦–ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆå¿…è¦ã«å¿œã˜ã¦ TARGETS_JSON ã§ä¸Šæ›¸ãï¼‰ ---
DEFAULT_TARGETS = [
    {
        "name": "ãƒãƒªã‚¢ãƒ¼",
        "site": "carsensor",
        "url": "https://www.carsensor.net/usedcar/bTO/index.html?CARC=TO_S114%2ATO_S115&SORT=22",
        "price_max": 3000000,
        "year_min": 2014,
        "mileage_max": 100000,
        "pages": 1,
    },
    {
        "name": "ãƒ•ã‚©ãƒ¬ã‚¹ã‚¿ãƒ¼",
        "site": "carsensor",
        "url": "https://www.carsensor.net/usedcar/bSU/index.html?CARC=SU_S042%2ASU_S043&SORT=22",
        "price_max": 2800000,
        "year_min": 2014,
        "mileage_max": 120000,
        "pages": 1,
    },
    {
        "name": "CX-5",
        "site": "carsensor",
        "url": "https://www.carsensor.net/usedcar/bMA/index.html?CARC=MA_S009&SORT=22",
        "price_max": 2500000,
        "year_min": 2015,
        "mileage_max": 100000,
        "pages": 1,
    },
    {
        "name": "ï¼ˆå‚è€ƒï¼‰ã‚°ãƒ¼ãƒãƒƒãƒˆ",
        "site": "goonet",
        "url": "https://www.goo-net.com/cgi-bin/fsearch/goo_used_search.cgi?category=USDN",
        "price_max": 99999999,
        "year_min": 2010,
        "mileage_max": 999999,
        "pages": 1,
    },
]

# --- æ­£è¦è¡¨ç¾ãƒ˜ãƒ«ãƒ‘ ---
_price_ja = re.compile(r"([0-9,.]+)\s*ä¸‡å††|([0-9,]+)\s*å††")
_km_ja    = re.compile(r"([0-9.]+)\s*ä¸‡?km")
_year_ja  = re.compile(r"(\d{4})å¹´")


def textnum_to_int(val: str) -> int:
    m = _price_ja.search(val)
    if not m:
        return 0
    if m.group(1):  # ä¸‡å††
        n = float(m.group(1).replace(",", "")) * 10000
        return int(n)
    return int(m.group(2).replace(",", ""))


def km_to_int(val: str) -> int:
    m = _km_ja.search(val)
    if not m:
        return 0
    s = m.group(1)
    if "ä¸‡" in val:
        return int(float(s) * 10000)
    return int(float(s))


def year_from_text(val: str) -> int:
    m = _year_ja.search(val)
    if not m:
        return 0
    return int(m.group(1))


# --- polite fetch ---
def fetch(url: str) -> str:
    time.sleep(1.2)  # è² è·é…æ…®
    r = requests.get(url, headers=HEADERS, timeout=20)
    r.raise_for_status()
    return r.text


# --- ã‚µã‚¤ãƒˆåˆ¥ãƒ‘ãƒ¼ã‚µ ---
def parse_carsensor_list(html: str) -> List[Dict[str, Any]]:
    soup = BeautifulSoup(html, "lxml")
    items: List[Dict[str, Any]] = []
    cards = soup.select(".cassette, .list, .cl-list-item, .cl-list-content") or []
    if not cards:
        cards = soup.select("a")
    for c in cards:
        try:
            a = c.select_one("h3 a") or c.select_one("a")
            if not a or not a.get("href"):
                continue
            title = a.get_text(strip=True)
            url = a["href"]
            if url.startswith("/"):
                url = "https://www.carsensor.net" + url
            t = c.get_text(" ", strip=True)
            price = textnum_to_int(t)
            year = year_from_text(t)
            mileage = km_to_int(t)
            if price == 0 and ("ä¾¡æ ¼" in t or "ä¸‡å††" in t):
                continue
            items.append({
                "title": title,
                "url": url,
                "year": year,
                "mileage": mileage,
                "price": price,
                "site": "carsensor",
            })
        except Exception:
            continue
    return items


def parse_goonet_list(html: str) -> List[Dict[str, Any]]:
    soup = BeautifulSoup(html, "lxml")
    items: List[Dict[str, Any]] = []
    rows = soup.select("a")
    for a in rows:
        href = a.get("href", "")
        if "/usedcar/" in href or "goo-net.com/usedcar" in href:
            title = a.get_text(strip=True)
            url = href if href.startswith("http") else "https://www.goo-net.com" + href
            t = a.get_text(" ", strip=True)
            items.append({
                "title": title,
                "url": url,
                "year": year_from_text(t),
                "mileage": km_to_int(t),
                "price": textnum_to_int(t),
                "site": "goonet",
            })
    return items


SITE_PARSERS = {
    "carsensor": parse_carsensor_list,
    "goonet":    parse_goonet_list,
}

# --- ç›¸å ´/åˆ¤å®šï¼ˆãƒ«ãƒ¼ãƒ«ï¼‰ ---
def _percentile(sorted_list, p: float):
    if not sorted_list:
        return None
    k = (len(sorted_list) - 1) * p
    f, c = math.floor(k), math.ceil(k)
    if f == c:
        return sorted_list[int(k)]
    return sorted_list[f] * (c - k) + sorted_list[c] * (k - f)


def compute_price_stats(items):
    prices = [it.get("price") for it in items if it.get("price")]
    if len(prices) < 4:
        return {"median": None, "q25": None}
    s = sorted(prices)
    return {"median": _percentile(s, 0.5), "q25": _percentile(s, 0.25)}


# --- åˆ†ä½å›å¸° OOF: p50 / p20 äºˆæ¸¬ ---
def _feat(it: Dict[str, Any]):
    title = (it.get("title") or "")
    return [
        it.get("year") or 0,
        it.get("mileage") or 0,
        1 if "ã‚µãƒ³ãƒ«ãƒ¼ãƒ•" in title else 0,
        1 if "ãƒ¬ã‚¶ãƒ¼" in title else 0,
        1 if "BOSE" in title else 0,
        1 if "JBL"  in title else 0,
    ]


def oof_quantile_preds(collected: List[Dict[str, Any]], alphas=(0.5, 0.2)):
    X = np.array([_feat(it) for it in collected], dtype=float)
    y = np.array([it.get("price") or 0 for it in collected], dtype=float)
    n = len(collected)
    if n < 12 or y.sum() == 0:
        return {a: [None]*n for a in alphas}
    kf = KFold(n_splits=3, shuffle=True, random_state=42)
    out = {a: np.zeros(n) for a in alphas}
    for a in alphas:
        preds = np.zeros(n)
        for tr, te in kf.split(X):
            mdl = GradientBoostingRegressor(loss="quantile", alpha=a, random_state=42)
            mdl.fit(X[tr], y[tr])
            preds[te] = mdl.predict(X[te])
        out[a] = preds
    return {a: v.tolist() for a, v in out.items()}


def assess_deal(it, stats, cfg):
    now = datetime.now().year
    price = it.get("price") or 0
    year  = it.get("year") or 0
    km    = it.get("mileage") or 0
    age   = max(0, now - year) if year else 15

    median = (stats or {}).get("median") or 0
    price_ratio = (price / median) if (price and median) else 1.0

    # ãƒ™ãƒ¼ã‚¹ï¼ˆä¾¡æ ¼æ¯”é‡è¦–ï¼‰
    if price_ratio <= 0.6:   base = 95
    elif price_ratio <= 0.7: base = 85
    elif price_ratio <= 0.8: base = 75
    elif price_ratio <= 0.9: base = 60
    elif price_ratio <= 1.0: base = 45
    else:                    base = 30

    # å¾®èª¿æ•´ï¼ˆå¹´å¼/è·é›¢/æ–‡å­—ï¼‰
    adj = 0
    if age <= 8: adj += 5
    if km and km <= 0.7 * cfg.get("mileage_max", 150_000): adj += 5
    title = (it.get("title") or "")
    if any(k in title for k in ["ã‚µãƒ³ãƒ«ãƒ¼ãƒ•", "ãƒ¬ã‚¶ãƒ¼", "BOSE", "JBL"]): adj += 3
    if any(k in title for k in ["ä¿®å¾©æ­´", "äº‹æ•…", "å† æ°´"]):               adj -= 20

    score = max(0, min(100, base + adj))

    # äºˆæ¸¬ãƒ™ãƒ¼ã‚¹ï¼ˆOOFï¼‰ãƒ–ãƒ¼ã‚¹ãƒˆ
    pred50 = it.get("pred_p50")
    pred20 = it.get("pred_p20")
    gap = None
    if isinstance(pred50, (int, float)) and pred50 > 0:
        gap = pred50 - price
        if gap > 500_000:
            score = min(100, score + 8)
        elif gap > 300_000:
            score = min(100, score + 5)
    if isinstance(pred20, (int, float)) and pred20 > 0 and price <= pred20:
        score = min(100, score + 5)

    # ç·Šæ€¥åº¦ï¼ˆ1-5ï¼‰
    if score >= 90:   urgency = 5
    elif score >= 80: urgency = 4
    elif score >= 70: urgency = 3
    elif score >= 60: urgency = 2
    else:             urgency = 1
    if price_ratio <= 0.6:
        urgency = min(5, urgency + 1)
    if gap is not None and gap > 500_000:
        urgency = min(5, urgency + 1)

    # ç›¸å ´ä¸æ˜ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆä¸­å¤®å€¤ãŒç„¡ã„å›ï¼‰
    if not median:
        if (price and price <= 0.8 * cfg.get("price_max", 9e9)
            and year and year >= cfg.get("year_min", 0) + 2
            and km and km <= 0.7 * cfg.get("mileage_max", 9e9)):
            urgency = max(urgency, 4)
            score   = max(score, 80)

    it["price_ratio"] = round(price_ratio, 2)
    it["score"]       = score
    it["urgency"]     = int(urgency)
    it["deal_gap"]    = int(gap) if gap is not None else None


# --- Discordé€šçŸ¥ ---
def discord_notify(items: List[Dict[str, Any]]):
    url = os.getenv("DISCORD_WEBHOOK_URL")
    if not url:
        print("[INFO] DISCORD_WEBHOOK_URL æœªè¨­å®šã€‚Discordé€šçŸ¥ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        return
    cands = [x for x in items if x.get("urgency", 1) >= 4][:5]
    if not cands:
        print("[INFO] Discordé€šçŸ¥å¯¾è±¡ãªã—ï¼ˆL4+ãªã—ï¼‰")
        return
    embeds = []
    for it in cands:
        price = f"{it.get('price',0):,}å††" if it.get('price') else "â€”"
        year  = it.get("year") or "â€”"
        km    = it.get("mileage") or 0
        km_s  = f"{km:,}km" if km else "â€”"
        gap   = it.get("deal_gap")
        gap_s = (f"+{gap:,}å††" if isinstance(gap, int) and gap is not None and gap >= 0
                 else (f"{gap:,}å††" if gap is not None else "â€”"))
        p50 = it.get("pred_p50"); p20 = it.get("pred_p20")
        p50s = f"{int(p50):,}å††" if isinstance(p50, (int,float)) else "â€”"
        p20s = f"{int(p20):,}å††" if isinstance(p20, (int,float)) else "â€”"
        embeds.append({
            "title": (it.get("title") or "")[:256],
            "url": it.get("url"),
            "description": f"{it.get('site','')} | {year}å¹´ | {km_s} | {price}",
            "fields": [
                {"name": "Score",       "value": str(it.get("score", 0)),             "inline": True},
                {"name": "Price Ratio", "value": str(it.get("price_ratio", '-')),     "inline": True},
                {"name": "Urgency",     "value": "ğŸ”¥" * it.get("urgency", 1),         "inline": True},
                {"name": "Deal Gap",    "value": gap_s,                                "inline": True},
                {"name": "p50(pred)",   "value": p50s,                                 "inline": True},
                {"name": "p20(pred)",   "value": p20s,                                 "inline": True},
            ],
        })
    payload = {
        "content": f"ğŸš— **ãŠã™ã™ã‚ä¸­å¤è»Šãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆå³è²·ã„å€™è£œï¼‰**\n{datetime.now():%Y-%m-%d %H:%M}",
        "embeds": embeds,
    }
    try:
        r = requests.post(url, json=payload, timeout=12)
        r.raise_for_status()
        print("[OK] Discord é€šçŸ¥å®Œäº†")
    except Exception as e:
        print(f"[WARN] Discord é€šçŸ¥å¤±æ•—: {e}")


# --- ãƒ¡ã‚¤ãƒ³ ---
def main():
    targets = DEFAULT_TARGETS
    tj = os.getenv("TARGETS_JSON")
    if tj:
        try:
            targets = json.loads(tj)
        except Exception as e:
            print(f"[WARN] TARGETS_JSON ã®èª­ã¿è¾¼ã¿å¤±æ•—: {e}")

    all_picks: List[Dict[str, Any]] = []

    for cfg in targets:
        site = cfg.get("site")
        url  = cfg.get("url")
        pages = int(cfg.get("pages", 1))
        parser = SITE_PARSERS.get(site)
        if not parser:
            print(f"[SKIP] æœªå¯¾å¿œã‚µã‚¤ãƒˆ: {site}")
            continue

        collected: List[Dict[str, Any]] = []
        for page in range(1, pages + 1):
            u = url + (f"&page={page}" if page > 1 else "")
            try:
                print(f"[GET] {u}")
                html = fetch(u)
                items = parser(html)
                collected.extend(items)
            except requests.HTTPError as e:
                code = getattr(e.response, "status_code", "?")
                print(f"[HTTP {code}] {u}")
            except Exception as e:
                print(f"[ERR] {u}: {e}")

        # åˆ†ä½å›å¸° OOF äºˆæ¸¬ï¼ˆååˆ†ãªä»¶æ•°ãŒã‚ã‚‹å›ã ã‘æœ‰åŠ¹ï¼‰
        qpreds = oof_quantile_preds(collected, alphas=(0.5, 0.2))
        for i, it in enumerate(collected):
            it["pred_p50"] = qpreds.get(0.5, [None]*len(collected))[i] if collected else None
            it["pred_p20"] = qpreds.get(0.2, [None]*len(collected))[i] if collected else None

        # ä¸­å¤®å€¤ç›¸å ´ â†’ è©•ä¾¡
        stats = compute_price_stats(collected)
        for it in collected:
            assess_deal(it, stats, cfg)

        # L4+ã‚’å„ªå…ˆã€è¶³ã‚Šãªã‘ã‚Œã°ã‚¹ã‚³ã‚¢ä¸Šä½ã§è£œå®Œ
        picks = [x for x in collected if x.get("urgency", 1) >= 4]
        if len(picks) < 8:
            extra = sorted([x for x in collected if x not in picks],
                           key=lambda x: x.get("score", 0), reverse=True)
            picks.extend(extra[:8 - len(picks)])

        print(f"  â†’ å€™è£œ {len(picks)} ä»¶ï¼ˆ{cfg.get('name')}ï¼‰")
        all_picks.extend(picks)

    # å…¨ä½“ã‹ã‚‰ä¸Šä½10
    all_picks.sort(key=lambda x: x.get("score", 0), reverse=True)
    top = all_picks[:10]

    # CSV å‡ºåŠ›
    out_csv = "results.csv"
    with open(out_csv, "w", newline="", encoding="utf-8-sig") as f:
        w = csv.writer(f)
        w.writerow(["title","url","site","year","mileage","price",
                    "score","price_ratio","urgency","pred_p50","pred_p20","deal_gap"])
        for it in top:
            p50 = it.get("pred_p50"); p20 = it.get("pred_p20")
            w.writerow([
                it.get("title", ""), it.get("url", ""), it.get("site", ""),
                it.get("year", 0), it.get("mileage", 0), it.get("price", 0),
                it.get("score", 0), it.get("price_ratio", "-"), it.get("urgency", 1),
                int(p50) if isinstance(p50, (int,float)) else "",
                int(p20) if isinstance(p20, (int,float)) else "",
                it.get("deal_gap","")
            ])
    print(f"[OK] CSV å‡ºåŠ›: {out_csv}ï¼ˆ{len(top)}ä»¶ï¼‰")

    # Discord é€šçŸ¥
    discord_notify(top)


if __name__ == "__main__":
    main()
