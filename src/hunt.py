#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Used-Car Scoutï¼ˆDiscordé€šçŸ¥ç‰ˆ / SUVç›£è¦–ãƒ»ãƒã‚¹ãƒ©ãƒ¼é™¤å¤– / ãƒ«ãƒ¼ãƒ« + åˆ†ä½å›å¸°ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼‰
- æ¤œç´¢ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ + ã‚¹ãƒãƒ›ç‰ˆ(STID=SMPH0001) + PAGEå·¡å›ã§ã€requests+BS4ã§å®‰å®šå–å¾—
- ä¸€è¦§ã§å¹´å¼/è·é›¢/ä¾¡æ ¼ãŒæ‹¾ãˆãªã„å ´åˆã€è©³ç´°ãƒšãƒ¼ã‚¸ã‚’1å›ã ã‘å‚ç…§ã—ã¦è£œå®Œï¼ˆè² è·ä½æ¸›ï¼‰
- ç›¸å ´1: åŒå›åãƒ‡ãƒ¼ã‚¿ã®ä¾¡æ ¼â€œä¸­å¤®å€¤â€ã‹ã‚‰ price_ratio ã‚’ç®—å‡ºï¼ˆå¸¸æ™‚ï¼‰
- ç›¸å ´2: ä»¶æ•°ååˆ†ãªå›ã¯ã€åˆ†ä½å›å¸°(Quantile GB, OOF)ã§ p50/p20 ã‚’æ¨å®š â†’ ãŠå¾—åº¦ãƒ–ãƒ¼ã‚¹ãƒˆ
- SUVã®ã¿æ‹¾ã† include_keywords / å¸¸æ™‚é™¤å¤–ã«ã€Œãƒã‚¹ãƒ©ãƒ¼/HUSTLERã€
- ä¸Šä½ã‚’ results.csv ã«ä¿å­˜
- Discordã¯äºŒæ®µé€šçŸ¥ï¼šğŸš€å³è²·ã„ï¼ˆãƒ‡ãƒ•ã‚©ç·Šæ€¥åº¦â‰§4ï¼‰ã¨ ğŸ¤”ã‚ã‚Šã‹ã‚‚ï¼ˆç·Šæ€¥åº¦3 or ã‚¹ã‚³ã‚¢70â€“84.9ï¼‰

ç’°å¢ƒå¤‰æ•°:
  DISCORD_WEBHOOK_URL_MAIN  â€¦ å³è²·ã„ãƒ¬ãƒ™ãƒ«ã®é€šçŸ¥å…ˆï¼ˆå¿…é ˆæ¨å¥¨ï¼‰
  DISCORD_WEBHOOK_URL_MAYBE â€¦ ã‚ã‚Šã‹ã‚‚ãƒ¬ãƒ™ãƒ«ã®é€šçŸ¥å…ˆï¼ˆä»»æ„ï¼‰
  DISCORD_WEBHOOK_URL       â€¦ æ—§å˜ä¸€Webhookåï¼ˆMAINæœªè¨­å®šæ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
  TARGETS_JSON              â€¦ ç›£è¦–ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä¸Šæ›¸ãï¼ˆJSONæ–‡å­—åˆ—ï¼‰
  DISCORD_DRY_RUN           â€¦ "1" ãªã‚‰é€šçŸ¥ã›ãšã€é€ä¿¡å†…å®¹ã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«è¡¨ç¤º

  IMMEDIATE_URGENCY_MIN     â€¦ å³è²·ã„ç·Šæ€¥åº¦ã—ãã„å€¤ï¼ˆæ—¢å®š 4ï¼‰
  MAYBE_SCORE_MIN           â€¦ ã‚ã‚Šã‹ã‚‚ã‚¹ã‚³ã‚¢ä¸‹é™ï¼ˆæ—¢å®š 70ï¼‰
  MAYBE_SCORE_MAX           â€¦ ã‚ã‚Šã‹ã‚‚ã‚¹ã‚³ã‚¢ä¸Šé™ï¼ˆæ—¢å®š 84.9ï¼‰
"""
from __future__ import annotations
import os, re, csv, json, time, math
from datetime import datetime
from typing import List, Dict, Any
from unicodedata import normalize
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

import requests
from bs4 import BeautifulSoup

import numpy as np
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import KFold

# --------- é€šä¿¡è¨­å®š ---------
UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/124.0 Safari/537.36"
)
HEADERS = {"User-Agent": UA, "Accept-Language": "ja,en;q=0.9"}

# --------- äºŒæ®µéšé€šçŸ¥ã®ã—ãã„å€¤ï¼ˆç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãå¯ï¼‰ ---------
IMMEDIATE_URGENCY_MIN = int(os.getenv("IMMEDIATE_URGENCY_MIN", "4"))   # å³è²·ã„: ç·Šæ€¥åº¦â‰§4
MAYBE_SCORE_MIN = float(os.getenv("MAYBE_SCORE_MIN", "70"))            # ã‚ã‚Šã‹ã‚‚: ã‚¹ã‚³ã‚¢ä¸‹é™
MAYBE_SCORE_MAX = float(os.getenv("MAYBE_SCORE_MAX", "84.9"))          # ã‚ã‚Šã‹ã‚‚: ä¸Šé™(å³è²·ã„æœªæº€)

# Webhookï¼ˆMAINã¯æ—§DISCORD_WEBHOOK_URLã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
WEBHOOK_MAIN  = os.getenv("DISCORD_WEBHOOK_URL_MAIN") or os.getenv("DISCORD_WEBHOOK_URL")
WEBHOOK_MAYBE = os.getenv("DISCORD_WEBHOOK_URL_MAYBE")
DRY_RUN = os.getenv("DISCORD_DRY_RUN", "0") == "1"

# --------- ç›£è¦–ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆå¿…è¦ã«å¿œã˜ã¦ TARGETS_JSON ã§ä¸Šæ›¸ãï¼‰ ---------
# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã‚ãªãŸãŒæç¤ºã—ãŸæ¤œç´¢URLã‚’ã‚µãƒ³ãƒ—ãƒ«ã§1ä»¶å…¥ã‚Œã¦ãŠãï¼ˆè»½ã‚ã« pages=2ï¼‰
DEFAULT_TARGETS = [
    {
        "name": "ãƒ†ã‚¹ãƒˆ: åŒ—æµ·é“SUV 2013å¹´ã€œ 7ä¸‡kmä»¥ä¸‹ ã€œ120ä¸‡å††",
        "site": "carsensor",
        "url": "https://www.carsensor.net/usedcar/search.php?STID=CS210610&YMIN=2013&AR=4&SMAX=70000&PMAX=1200000&SP=D&NOTKEI=1&BT=X",
        "pages": 2,
        "price_max": 1200000,
        "year_min": 2013,
        "mileage_max": 70000,
        "include_keywords": [],  # URLå´ã§æ¡ä»¶æŒ‡å®šã—ã¦ã„ã‚‹ã®ã§ç©ºã§OK
        "exclude_keywords": []
    }
]

# --------- æ­£è¦è¡¨ç¾ãƒ˜ãƒ«ãƒ‘ ---------
_price_ja = re.compile(r"([0-9,.]+)\s*ä¸‡å††|([0-9,]+)\s*å††")
_km_ja    = re.compile(r"([0-9.]+)\s*ä¸‡?km")
_year_ja  = re.compile(r"(\d{4})å¹´")

def textnum_to_int(val: str) -> int:
    m = _price_ja.search(val)
    if not m:
        return 0
    if m.group(1):  # ä¸‡å††
        n = float(m.group(1).replace(",", "")) * 10000
        return int(n)
    return int(m.group(2).replace(",", ""))

def km_to_int(val: str) -> int:
    m = _km_ja.search(val)
    if not m:
        return 0
    s = m.group(1)
    if "ä¸‡" in val:
        return int(float(s) * 10000)
    return int(float(s))

def year_from_text(val: str) -> int:
    m = _year_ja.search(val)
    if not m:
        return 0
    return int(m.group(1))

# --------- URLè£œæ­£ï¼ˆã‚¹ãƒãƒ›ç‰ˆ/PAGEä»˜ä¸ï¼‰ ---------
def _ensure_mobile_url(url: str) -> str:
    """Carsensoræ¤œç´¢URLã‚’ã‚¹ãƒãƒ›ç‰ˆã«å¼·åˆ¶(STID=SMPH0001)ã€‚AL=1ã¯è¡¨è¨˜å®‰å®šåŒ–ç”¨ã€‚"""
    p = urlparse(url)
    q = parse_qs(p.query)
    q['STID'] = ['SMPH0001']  # ã‚¹ãƒãƒ›ç‰ˆ
    q.setdefault('AL', ['1'])
    new_q = urlencode(q, doseq=True)
    return urlunparse(p._replace(query=new_q))

def _with_page(url: str, page: int) -> str:
    """PAGE=n ã‚’ä»˜ä¸"""
    p = urlparse(url)
    q = parse_qs(p.query)
    q['PAGE'] = [str(page)]
    new_q = urlencode(q, doseq=True)
    return urlunparse(p._replace(query=new_q))

# --------- polite fetch ---------
def fetch(url: str) -> str:
    time.sleep(1.2)  # è² è·é…æ…®ï¼ˆå¿…è¦ã«å¿œã˜ã¦ãƒ©ãƒ³ãƒ€ãƒ åŒ–ï¼‰
    r = requests.get(url, headers=HEADERS, timeout=20)
    r.raise_for_status()
    return r.text

# --------- ã‚µã‚¤ãƒˆåˆ¥ãƒ‘ãƒ¼ã‚µï¼ˆã‚¹ãƒãƒ›ç‰ˆå‰æï¼‰ ---------
def parse_carsensor_list(html: str) -> List[Dict[str, Any]]:
    """ã‚¹ãƒãƒ›ç‰ˆã§ /usedcar/detail/ ã¸ã®ãƒªãƒ³ã‚¯ã®ã¿æ‹¾ã„ã€è¦ªãƒ–ãƒ­ãƒƒã‚¯ã®æ–‡å­—ã‹ã‚‰æ•°å€¤æŠ½å‡º"""
    soup = BeautifulSoup(html, "lxml")
    items: List[Dict[str, Any]] = []
    seen = set()

    for a in soup.select('a[href*="/usedcar/detail/"]'):
        href = a.get("href", "")
        if not href:
            continue
        url = href if href.startswith("http") else "https://www.carsensor.net" + href
        if url in seen:
            continue
        seen.add(url)

        title = a.get_text(" ", strip=True)

        # è¦ªã‚’æ•°éšå±¤ãŸã©ã£ã¦ã€ã¾ã¨ã¾ã‚Šã®ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        block = a
        for _ in range(4):
            if block and block.parent:
                block = block.parent
            else:
                break
        text = (block.get_text(" ", strip=True) if block else a.get_text(" ", strip=True))

        items.append({
            "title": title,
            "url": url,
            "year": year_from_text(text),
            "mileage": km_to_int(text),
            "price": textnum_to_int(text),
            "site": "carsensor",
        })
    return items

def parse_goonet_list(html: str) -> List[Dict[str, Any]]:
    # ä½¿ã†ãªã‚‰åŒæ§˜ã«å®Ÿè£…ã€‚ä»Šå›ã¯æœªä½¿ç”¨ã€‚
    soup = BeautifulSoup(html, "lxml")
    out: List[Dict[str, Any]] = []
    for a in soup.select('a[href*="/usedcar/"]'):
        href = a.get("href", "")
        if not href:
            continue
        url = href if href.startswith("http") else "https://www.goo-net.com" + href
        t = a.get_text(" ", strip=True)
        out.append({
            "title": a.get_text(strip=True),
            "url": url,
            "year": year_from_text(t),
            "mileage": km_to_int(t),
            "price": textnum_to_int(t),
            "site": "goonet",
        })
    return out

SITE_PARSERS = {
    "carsensor": parse_carsensor_list,
    "goonet":    parse_goonet_list,
}

# --------- è©³ç´°ãƒšãƒ¼ã‚¸è£œå®Œ ---------
def enrich_from_detail(it: Dict[str, Any]) -> None:
    """ä¸€è¦§ã§æŠœã‘ãŸ year/mileage/price ã‚’è©³ç´°HTMLã‹ã‚‰è£œå®Œ"""
    try:
        h = fetch(it["url"])
    except Exception:
        return
    if not it.get("year"):
        it["year"] = year_from_text(h) or it.get("year", 0)
    if not it.get("mileage"):
        it["mileage"] = km_to_int(h) or it.get("mileage", 0)
    if not it.get("price"):
        it["price"] = textnum_to_int(h) or it.get("price", 0)

# --------- ã‚¿ã‚¤ãƒˆãƒ«æ­£è¦åŒ– & ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆSUVã ã‘/ãƒã‚¹ãƒ©ãƒ¼é™¤å¤–ï¼‰ ---------
def _norm(s: str) -> str:
    return normalize("NFKC", (s or "")).upper()

def keyword_filter(items, include_keywords=None, exclude_keywords=None):
    inc = [_norm(k) for k in (include_keywords or [])]
    exc = {_norm(k) for k in ((exclude_keywords or []) + ["ãƒã‚¹ãƒ©ãƒ¼", "HUSTLER"])}
    out = []
    for it in items:
        title_n = _norm(it.get("title", ""))
        if inc and not any(k in title_n for k in inc):
            continue
        if any(k in title_n for k in exc):
            continue
        out.append(it)
    return out

# --------- ç›¸å ´/åˆ¤å®šï¼ˆãƒ«ãƒ¼ãƒ«ï¼‰ ---------
def _percentile(sorted_list, p: float):
    if not sorted_list:
        return None
    k = (len(sorted_list) - 1) * p
    f, c = math.floor(k), math.ceil(k)
    if f == c:
        return sorted_list[int(k)]
    return sorted_list[f] * (c - k) + sorted_list[c] * (k - f)

def compute_price_stats(items):
    prices = [it.get("price") for it in items if it.get("price")]
    if len(prices) < 4:
        return {"median": None, "q25": None}
    s = sorted(prices)
    return {"median": _percentile(s, 0.5), "q25": _percentile(s, 0.25)}

# --------- åˆ†ä½å›å¸° OOF: p50 / p20 äºˆæ¸¬ ---------
def _feat(it: Dict[str, Any]):
    title = (it.get("title") or "")
    return [
        it.get("year") or 0,
        it.get("mileage") or 0,
        1 if "ã‚µãƒ³ãƒ«ãƒ¼ãƒ•" in title else 0,
        1 if "ãƒ¬ã‚¶ãƒ¼" in title else 0,
        1 if "BOSE" in title else 0,
        1 if "JBL"  in title else 0,
    ]

def oof_quantile_preds(collected: List[Dict[str, Any]], alphas=(0.5, 0.2)):
    X = np.array([_feat(it) for it in collected], dtype=float)
    y = np.array([it.get("price") or 0 for it in collected], dtype=float)
    n = len(collected)
    if n < 12 or y.sum() == 0:
        return {a: [None]*n for a in alphas}
    kf = KFold(n_splits=3, shuffle=True, random_state=42)
    out = {a: np.zeros(n) for a in alphas}
    for a in alphas:
        preds = np.zeros(n)
        for tr, te in kf.split(X):
            mdl = GradientBoostingRegressor(loss="quantile", alpha=a, random_state=42)
            mdl.fit(X[tr], y[tr])
            preds[te] = mdl.predict(X[te])
        out[a] = preds
    return {a: v.tolist() for a, v in out.items()}

def assess_deal(it, stats, cfg):
    now = datetime.now().year
    price = it.get("price") or 0
    year  = it.get("year") or 0
    km    = it.get("mileage") or 0
    age   = max(0, now - year) if year else 15

    median = (stats or {}).get("median") or 0
    price_ratio = (price / median) if (price and median) else 1.0

    # ãƒ™ãƒ¼ã‚¹ï¼ˆä¾¡æ ¼æ¯”é‡è¦–ï¼‰
    if price_ratio <= 0.6:   base = 95
    elif price_ratio <= 0.7: base = 85
    elif price_ratio <= 0.8: base = 75
    elif price_ratio <= 0.9: base = 60
    elif price_ratio <= 1.0: base = 45
    else:                    base = 30

    # å¾®èª¿æ•´ï¼ˆå¹´å¼/è·é›¢/æ–‡å­—ï¼‰
    adj = 0
    if age <= 8: adj += 5
    if km and km <= 0.7 * cfg.get("mileage_max", 150_000): adj += 5
    title = (it.get("title") or "")
    if any(k in title for k in ["ã‚µãƒ³ãƒ«ãƒ¼ãƒ•", "ãƒ¬ã‚¶ãƒ¼", "BOSE", "JBL"]): adj += 3
    if any(k in title for k in ["ä¿®å¾©æ­´", "äº‹æ•…", "å† æ°´"]):               adj -= 20

    score = max(0, min(100, base + adj))

    # äºˆæ¸¬ãƒ™ãƒ¼ã‚¹ï¼ˆOOFï¼‰ãƒ–ãƒ¼ã‚¹ãƒˆ
    pred50 = it.get("pred_p50")
    pred20 = it.get("pred_p20")
    gap = None
    if isinstance(pred50, (int, float)) and pred50 > 0:
        gap = pred50 - price
        if gap > 500_000:
            score = min(100, score + 8)
        elif gap > 300_000:
            score = min(100, score + 5)
    if isinstance(pred20, (int, float)) and pred20 > 0 and price <= pred20:
        score = min(100, score + 5)

    # ç·Šæ€¥åº¦ï¼ˆ1-5ï¼‰
    if score >= 90:   urgency = 5
    elif score >= 80: urgency = 4
    elif score >= 70: urgency = 3
    elif score >= 60: urgency = 2
    else:             urgency = 1
    if price_ratio <= 0.6:
        urgency = min(5, urgency + 1)
    if gap is not None and gap > 500_000:
        urgency = min(5, urgency + 1)

    # ç›¸å ´ä¸æ˜ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆä¸­å¤®å€¤ãŒç„¡ã„å›ï¼‰
    if not median:
        if (price and price <= 0.8 * cfg.get("price_max", 9e9)
            and year and year >= cfg.get("year_min", 0) + 2
            and km and km <= 0.7 * cfg.get("mileage_max", 9e9)):
            urgency = max(urgency, 4)
            score   = max(score, 80)

    it["price_ratio"] = round(price_ratio, 2) if median else "-"
    it["score"]       = score
    it["urgency"]     = int(urgency)
    it["deal_gap"]    = int(gap) if gap is not None else None

# --------- Discordé€šçŸ¥ï¼ˆå…±é€šï¼‰ ---------
def _post_discord(items: List[Dict[str, Any]], webhook_url: str | None, title_prefix: str):
    if not items:
        print(f"[INFO] {title_prefix} é€šçŸ¥å¯¾è±¡ãªã—")
        return
    if DRY_RUN:
        preview = [{
            "title": it.get("title"),
            "url": it.get("url"),
            "price": it.get("price"),
            "year": it.get("year"),
            "mileage": it.get("mileage"),
            "score": it.get("score"),
            "urgency": it.get("urgency"),
            "price_ratio": it.get("price_ratio"),
            "deal_gap": it.get("deal_gap"),
        } for it in items]
        print(f"[DRY-RUN] {title_prefix} payload preview:", json.dumps(preview, ensure_ascii=False, indent=2))
        return
    if not webhook_url:
        print(f"[INFO] {title_prefix} ç”¨Webhookæœªè¨­å®šã€‚é€šçŸ¥ã‚¹ã‚­ãƒƒãƒ—")
        return

    embeds = []
    for it in items[:5]:  # å„ã‚«ãƒ†ã‚´ãƒªæœ€å¤§5ä»¶
        price = f"{it.get('price',0):,}å††" if it.get('price') else "â€”"
        year  = it.get("year") or "â€”"
        km    = it.get("mileage") or 0
        km_s  = f"{km:,}km" if km else "â€”"
        gap   = it.get("deal_gap")
        gap_s = (f"+{gap:,}å††" if isinstance(gap, int) and gap is not None and gap >= 0
                 else (f"{gap:,}å††" if gap is not None else "â€”"))
        p50 = it.get("pred_p50"); p20 = it.get("pred_p20")
        p50s = f"{int(p50):,}å††" if isinstance(p50, (int,float)) else "â€”"
        p20s = f"{int(p20):,}å††" if isinstance(p20, (int,float)) else "â€”"
        embeds.append({
            "title": (it.get("title") or "")[:256],
            "url": it.get("url"),
            "description": f"{it.get('site','')} | {year}å¹´ | {km_s} | {price}",
            "fields": [
                {"name": "Score",       "value": str(it.get("score", 0)),         "inline": True},
                {"name": "Price Ratio", "value": str(it.get("price_ratio", '-')), "inline": True},
                {"name": "Urgency",     "value": "ğŸ”¥" * it.get("urgency", 1),     "inline": True},
                {"name": "Deal Gap",    "value": gap_s,                            "inline": True},
                {"name": "p50(pred)",   "value": p50s,                             "inline": True},
                {"name": "p20(pred)",   "value": p20s,                             "inline": True},
            ],
        })
    payload = {
        "content": f"{title_prefix}\n{datetime.now():%Y-%m-%d %H:%M}",
        "embeds": embeds,
    }
    try:
        r = requests.post(webhook_url, json=payload, timeout=12)
        r.raise_for_status()
        print(f"[OK] Discord é€šçŸ¥å®Œäº†: {title_prefix}")
    except Exception as e:
        print(f"[WARN] Discord é€šçŸ¥å¤±æ•—({title_prefix}): {e}")

# å¾Œæ–¹äº’æ›ã®å˜ä¸€é€šçŸ¥ï¼ˆå¿…è¦ãªã‚‰åˆ©ç”¨ï¼‰
def discord_notify(items: List[Dict[str, Any]]):
    url = WEBHOOK_MAIN  # æ—§: DISCORD_WEBHOOK_URL ã‚’å«ã‚€
    if DRY_RUN:
        preview = [{
            "title": it.get("title"),
            "url": it.get("url"),
            "price": it.get("price"),
            "year": it.get("year"),
            "mileage": it.get("mileage"),
            "score": it.get("score"),
            "urgency": it.get("urgency"),
            "price_ratio": it.get("price_ratio"),
            "deal_gap": it.get("deal_gap"),
        } for it in items if it.get("urgency",1) >= IMMEDIATE_URGENCY_MIN]
        print("[DRY-RUN] (legacy) Discord payload preview:", json.dumps(preview, ensure_ascii=False, indent=2))
        return
    if not url:
        print("[INFO] DISCORD_WEBHOOK_URL æœªè¨­å®šã€‚Discordé€šçŸ¥ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        return
    cands = [x for x in items if x.get("urgency", 1) >= IMMEDIATE_URGENCY_MIN][:5]
    if not cands:
        print("[INFO] Discordé€šçŸ¥å¯¾è±¡ãªã—ï¼ˆå³è²·ã„è©²å½“ãªã—ï¼‰")
        return
    _post_discord(cands, url, "ğŸš€ å³è²·ã„ãƒ¬ãƒ™ãƒ«ï¼ˆlegacyï¼‰")

# --------- å€™è£œã®åˆ†å‰²ï¼ˆå³è²·ã„ / ã‚ã‚Šã‹ã‚‚ï¼‰ ---------
def split_candidates(all_items: List[Dict[str, Any]]):
    immediate = [x for x in all_items if x.get("urgency",1) >= IMMEDIATE_URGENCY_MIN]
    maybe = [x for x in all_items
             if (x.get("urgency",1) == 3) or
                (MAYBE_SCORE_MIN <= x.get("score",0) <= MAYBE_SCORE_MAX)]
    ids = set(id(x) for x in immediate)
    maybe = [x for x in maybe if id(x) not in ids]
    immediate.sort(key=lambda x: x.get("score",0), reverse=True)
    maybe.sort(key=lambda x: x.get("score",0), reverse=True)
    return immediate[:5], maybe[:5]

# --------- ãƒ¡ã‚¤ãƒ³ ---------
def main():
    targets = DEFAULT_TARGETS
    tj = os.getenv("TARGETS_JSON")
    if tj:
        try:
            targets = json.loads(tj)
        except Exception as e:
            print(f"[WARN] TARGETS_JSON ã®èª­ã¿è¾¼ã¿å¤±æ•—: {e}")

    all_picks: List[Dict[str, Any]] = []
    all_items: List[Dict[str, Any]] = []

    for cfg in targets:
        site = cfg.get("site")
        url  = cfg.get("url")
        pages = int(cfg.get("pages", 1))
        parser = SITE_PARSERS.get(site)
        if not parser:
            print(f"[SKIP] æœªå¯¾å¿œã‚µã‚¤ãƒˆ: {site}")
            continue

        collected: List[Dict[str, Any]] = []
        mobile_base = _ensure_mobile_url(url)

        for page in range(1, pages + 1):
            u = _with_page(mobile_base, page)
            try:
                print(f"[GET] {u}")
                html = fetch(u)
                items = parser(html)
                # SUVã®ã¿ + ãƒã‚¹ãƒ©ãƒ¼é™¤å¤–ï¼ˆè¡¨è¨˜ã‚†ã‚Œå¸åï¼‰
                items = keyword_filter(
                    items,
                    include_keywords=cfg.get("include_keywords"),
                    exclude_keywords=cfg.get("exclude_keywords")
                )

                # è¶³ã‚Šãªã„æ•°å€¤ã¯è©³ç´°1å›ã§è£œå®Œï¼ˆå–ã‚Šã™ãé˜²æ­¢ã§ä¸Šä½30ä»¶ã¾ã§ï¼‰
                need_enrich = [x for x in items if (not x.get("price") or not x.get("year") or not x.get("mileage"))]
                for it in need_enrich[:30]:
                    enrich_from_detail(it)

                collected.extend(items)
                print(f"  â”” parsed {len(items)} items (page {page})")
            except requests.HTTPError as e:
                code = getattr(e.response, "status_code", "?")
                print(f"[HTTP {code}] {u}")
            except Exception as e:
                print(f"[ERR] {u}: {e}")

        # åˆ†ä½å›å¸° OOF äºˆæ¸¬ï¼ˆååˆ†ãªä»¶æ•°ãŒã‚ã‚‹å›ã ã‘ï¼‰
        qpreds = oof_quantile_preds(collected, alphas=(0.5, 0.2))
        for i, it in enumerate(collected):
            it["pred_p50"] = qpreds.get(0.5, [None]*len(collected))[i] if collected else None
            it["pred_p20"] = qpreds.get(0.2, [None]*len(collected))[i] if collected else None

        # ä¸­å¤®å€¤ç›¸å ´ â†’ è©•ä¾¡
        stats = compute_price_stats(collected)
        for it in collected:
            assess_deal(it, stats, cfg)

        all_items.extend(collected)

        # CSVç”¨ã®ä¸Šä½å€™è£œï¼ˆL4+å„ªå…ˆã€è¶³ã‚Šãªã‘ã‚Œã°ã‚¹ã‚³ã‚¢ã§è£œå®Œï¼‰
        picks = [x for x in collected if x.get("urgency", 1) >= 4]
        if len(picks) < 8:
            extra = sorted([x for x in collected if x not in picks],
                           key=lambda x: x.get("score", 0), reverse=True)
            picks.extend(extra[:8 - len(picks)])
        print(f"  â†’ å€™è£œ {len(picks)} ä»¶ï¼ˆ{cfg.get('name')}ï¼‰")
        all_picks.extend(picks)

    # å…¨ä½“ã‹ã‚‰ä¸Šä½10ã‚’æ›¸ãå‡ºã—ï¼ˆäº’æ›ï¼‰
    all_picks.sort(key=lambda x: x.get("score", 0), reverse=True)
    top = all_picks[:10]

    out_csv = "results.csv"
    with open(out_csv, "w", newline="", encoding="utf-8-sig") as f:
        w = csv.writer(f)
        w.writerow(["title","url","site","year","mileage","price",
                    "score","price_ratio","urgency","pred_p50","pred_p20","deal_gap"])
        for it in top:
            p50 = it.get("pred_p50"); p20 = it.get("pred_p20")
            w.writerow([
                it.get("title", ""), it.get("url", ""), it.get("site", ""),
                it.get("year", 0), it.get("mileage", 0), it.get("price", 0),
                it.get("score", 0), it.get("price_ratio", "-"), it.get("urgency", 1),
                int(p50) if isinstance(p50, (int,float)) else "",
                int(p20) if isinstance(p20, (int,float)) else "",
                it.get("deal_gap","")
            ])
    print(f"[OK] CSV å‡ºåŠ›: {out_csv}ï¼ˆ{len(top)}ä»¶ï¼‰")

    # äºŒæ®µéšé€šçŸ¥
    immediate, maybe = split_candidates(all_items)
    _post_discord(immediate, WEBHOOK_MAIN,  "ğŸš€ å³è²·ã„ãƒ¬ãƒ™ãƒ«")
    _post_discord(maybe,    WEBHOOK_MAYBE, "ğŸ¤” ã‚ã‚Šã‹ã‚‚ãƒ¬ãƒ™ãƒ«")

    # äº’æ›ã®å˜ä¸€é€šçŸ¥ã‚’ä½¿ã†å ´åˆã¯å¿…è¦ã«å¿œã˜ã¦
    # discord_notify(top)

if __name__ == "__main__":
    main()
