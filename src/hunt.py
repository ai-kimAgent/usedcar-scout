#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Used-Car ScoutÔºàDiscordÈÄöÁü•Áâà / SUVÁõ£Ë¶ñ„Éª„Éè„Çπ„É©„ÉºÈô§Â§ñ / „É´„Éº„É´ + ÂàÜ‰ΩçÂõûÂ∏∞„Éè„Ç§„Éñ„É™„ÉÉ„ÉâÔºâ
- Ê§úÁ¥¢„Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà + „Çπ„Éû„ÉõÁâà(STID=SMPH0001) + PAGEÂ∑°Âõû„Åß„ÄÅrequests+BS4„ÅßÂÆâÂÆöÂèñÂæó
- ‰∏ÄË¶ß„ÅßÂπ¥Âºè/Ë∑ùÈõ¢/‰æ°Ê†º„ÅåÊãæ„Åà„Å™„ÅÑÂ†¥Âêà„ÄÅË©≥Á¥∞„Éö„Éº„Ç∏„Çí1Âõû„Å†„ÅëÂèÇÁÖß„Åó„Å¶Ë£úÂÆåÔºàË≤†Ëç∑‰ΩéÊ∏õÔºâ
- Áõ∏Â†¥1: ÂêåÂõûÂèé„Éá„Éº„Çø„ÅÆ‰æ°Ê†º‚Äú‰∏≠Â§ÆÂÄ§‚Äù„Åã„Çâ price_ratio „ÇíÁÆóÂá∫ÔºàÂ∏∏ÊôÇÔºâ
- Áõ∏Â†¥2: ‰ª∂Êï∞ÂçÅÂàÜ„Å™Âõû„ÅØ„ÄÅÂàÜ‰ΩçÂõûÂ∏∞(Quantile GB, OOF)„Åß p50/p20 „ÇíÊé®ÂÆö ‚Üí „ÅäÂæóÂ∫¶„Éñ„Éº„Çπ„Éà
- SUV„ÅÆ„ÅøÊãæ„ÅÜ include_keywords / Â∏∏ÊôÇÈô§Â§ñ„Å´„Äå„Éè„Çπ„É©„Éº/HUSTLER„Äç
- ‰∏ä‰Ωç„Çí results.csv „Å´‰øùÂ≠ò
- Discord„ÅØ‰∫åÊÆµÈÄöÁü•ÔºöüöÄÂç≥Ë≤∑„ÅÑÔºà„Éá„Éï„Ç©Á∑äÊÄ•Â∫¶‚âß4Ôºâ„Å® ü§î„ÅÇ„Çä„Åã„ÇÇÔºàÁ∑äÊÄ•Â∫¶3 or „Çπ„Ç≥„Ç¢70‚Äì84.9Ôºâ

Áí∞Â¢ÉÂ§âÊï∞:
  DISCORD_WEBHOOK_URL_MAIN  ‚Ä¶ Âç≥Ë≤∑„ÅÑ„É¨„Éô„É´„ÅÆÈÄöÁü•ÂÖàÔºàÂøÖÈ†àÊé®Â•®Ôºâ
  DISCORD_WEBHOOK_URL_MAYBE ‚Ä¶ „ÅÇ„Çä„Åã„ÇÇ„É¨„Éô„É´„ÅÆÈÄöÁü•ÂÖàÔºà‰ªªÊÑèÔºâ
  DISCORD_WEBHOOK_URL       ‚Ä¶ ÊóßÂçò‰∏ÄWebhookÂêçÔºàMAINÊú™Ë®≠ÂÆöÊôÇ„ÅÆ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºâ
  TARGETS_JSON              ‚Ä¶ Áõ£Ë¶ñ„Çø„Éº„Ç≤„ÉÉ„Éà‰∏äÊõ∏„ÅçÔºàJSONÊñáÂ≠óÂàóÔºâ
  DISCORD_DRY_RUN           ‚Ä¶ "1" „Å™„ÇâÈÄöÁü•„Åõ„Åö„ÄÅÈÄÅ‰ø°ÂÜÖÂÆπ„Çí„Ç≥„É≥„ÇΩ„Éº„É´Ë°®Á§∫

  IMMEDIATE_URGENCY_MIN     ‚Ä¶ Âç≥Ë≤∑„ÅÑÁ∑äÊÄ•Â∫¶„Åó„Åç„ÅÑÂÄ§ÔºàÊó¢ÂÆö 4Ôºâ
  MAYBE_SCORE_MIN           ‚Ä¶ „ÅÇ„Çä„Åã„ÇÇ„Çπ„Ç≥„Ç¢‰∏ãÈôêÔºàÊó¢ÂÆö 70Ôºâ
  MAYBE_SCORE_MAX           ‚Ä¶ „ÅÇ„Çä„Åã„ÇÇ„Çπ„Ç≥„Ç¢‰∏äÈôêÔºàÊó¢ÂÆö 84.9Ôºâ
"""
from __future__ import annotations
import os, re, csv, json, time, math
from datetime import datetime
from typing import List, Dict, Any
from unicodedata import normalize
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

import requests
from bs4 import BeautifulSoup

import numpy as np
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import KFold

# --------- ÈÄö‰ø°Ë®≠ÂÆö ---------
UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/124.0 Safari/537.36"
)
HEADERS = {"User-Agent": UA, "Accept-Language": "ja,en;q=0.9"}

# --------- ‰∫åÊÆµÈöéÈÄöÁü•„ÅÆ„Åó„Åç„ÅÑÂÄ§ÔºàÁí∞Â¢ÉÂ§âÊï∞„Åß‰∏äÊõ∏„ÅçÂèØÔºâ ---------
IMMEDIATE_URGENCY_MIN = int(os.getenv("IMMEDIATE_URGENCY_MIN", "4"))   # Âç≥Ë≤∑„ÅÑ: Á∑äÊÄ•Â∫¶‚âß4
MAYBE_SCORE_MIN = float(os.getenv("MAYBE_SCORE_MIN", "70"))            # „ÅÇ„Çä„Åã„ÇÇ: „Çπ„Ç≥„Ç¢‰∏ãÈôê
MAYBE_SCORE_MAX = float(os.getenv("MAYBE_SCORE_MAX", "84.9"))          # „ÅÇ„Çä„Åã„ÇÇ: ‰∏äÈôê(Âç≥Ë≤∑„ÅÑÊú™Ê∫Ä)

# WebhookÔºàMAIN„ÅØÊóßDISCORD_WEBHOOK_URL„Çí„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºâ
WEBHOOK_MAIN  = os.getenv("DISCORD_WEBHOOK_URL_MAIN") or os.getenv("DISCORD_WEBHOOK_URL")
WEBHOOK_MAYBE = os.getenv("DISCORD_WEBHOOK_URL_MAYBE")
DRY_RUN = os.getenv("DISCORD_DRY_RUN", "0") == "1"

# --------- Áõ£Ë¶ñ„Çø„Éº„Ç≤„ÉÉ„ÉàÔºàÂøÖË¶Å„Å´Âøú„Åò„Å¶ TARGETS_JSON „Åß‰∏äÊõ∏„ÅçÔºâ ---------
# „Éá„Éï„Ç©„É´„Éà„ÅØ„ÅÇ„Å™„Åü„ÅåÊèêÁ§∫„Åó„ÅüÊ§úÁ¥¢URL„Çí„Çµ„É≥„Éó„É´„Åß1‰ª∂ÂÖ•„Çå„Å¶„Åä„ÅèÔºàËªΩ„ÇÅ„Å´ pages=2Ôºâ
DEFAULT_TARGETS = [
    {
        "name": "„ÉÜ„Çπ„Éà: ÂåóÊµ∑ÈÅìSUV 2013Âπ¥„Äú 7‰∏ákm‰ª•‰∏ã „Äú120‰∏áÂÜÜ",
        "site": "carsensor",
        "url": "https://www.carsensor.net/usedcar/search.php?STID=CS210610&YMIN=2013&AR=4&SMAX=70000&PMAX=1200000&SP=D&NOTKEI=1&BT=X",
        "pages": 2,
        "price_max": 1200000,
        "year_min": 2013,
        "mileage_max": 70000,
        "include_keywords": [],  # URLÂÅ¥„ÅßÊù°‰ª∂ÊåáÂÆö„Åó„Å¶„ÅÑ„Çã„ÅÆ„ÅßÁ©∫„ÅßOK
        "exclude_keywords": []
    }
]

# --------- Ê≠£Ë¶èË°®Áèæ„Éò„É´„Éë ---------
_price_ja = re.compile(r"([0-9,.]+)\s*‰∏áÂÜÜ|([0-9,]+)\s*ÂÜÜ")
_km_ja    = re.compile(r"([0-9.]+)\s*‰∏á?km")
_year_ja  = re.compile(r"(\d{4})Âπ¥")

def textnum_to_int(val: str) -> int:
    m = _price_ja.search(val)
    if not m:
        return 0
    if m.group(1):  # ‰∏áÂÜÜ
        n = float(m.group(1).replace(",", "")) * 10000
        return int(n)
    return int(m.group(2).replace(",", ""))

def km_to_int(val: str) -> int:
    m = _km_ja.search(val)
    if not m:
        return 0
    s = m.group(1)
    if "‰∏á" in val:
        return int(float(s) * 10000)
    return int(float(s))

def year_from_text(val: str) -> int:
    m = _year_ja.search(val)
    if not m:
        return 0
    return int(m.group(1))

# --------- URLË£úÊ≠£Ôºà„Çπ„Éû„ÉõÁâà/PAGE‰ªò‰∏éÔºâ ---------
def _ensure_mobile_url(url: str) -> str:
    """CarsensorÊ§úÁ¥¢URL„Çí„Çπ„Éû„ÉõÁâà„Å´Âº∑Âà∂(STID=SMPH0001)„ÄÇAL=1„ÅØË°®Ë®òÂÆâÂÆöÂåñÁî®„ÄÇ"""
    p = urlparse(url)
    q = parse_qs(p.query)
    q['STID'] = ['SMPH0001']  # „Çπ„Éû„ÉõÁâà
    q.setdefault('AL', ['1'])
    new_q = urlencode(q, doseq=True)
    return urlunparse(p._replace(query=new_q))

def _with_page(url: str, page: int) -> str:
    """PAGE=n „Çí‰ªò‰∏é"""
    p = urlparse(url)
    q = parse_qs(p.query)
    q['PAGE'] = [str(page)]
    new_q = urlencode(q, doseq=True)
    return urlunparse(p._replace(query=new_q))

# --------- polite fetch ---------
def fetch(url: str) -> str:
    time.sleep(1.2)  # Ë≤†Ëç∑ÈÖçÊÖÆÔºàÂøÖË¶Å„Å´Âøú„Åò„Å¶„É©„É≥„ÉÄ„É†ÂåñÔºâ
    r = requests.get(url, headers=HEADERS, timeout=20)
    r.raise_for_status()
    return r.text

# --------- „Çµ„Ç§„ÉàÂà•„Éë„Éº„ÇµÔºà„Çπ„Éû„ÉõÁâàÂâçÊèêÔºâ ---------
def parse_carsensor_list(html: str) -> List[Dict[str, Any]]:
    """„Çπ„Éû„ÉõÁâà„Åß /usedcar/detail/ „Å∏„ÅÆ„É™„É≥„ÇØ„ÅÆ„ÅøÊãæ„ÅÑ„ÄÅË¶™„Éñ„É≠„ÉÉ„ÇØ„ÅÆÊñáÂ≠ó„Åã„ÇâÊï∞ÂÄ§ÊäΩÂá∫"""
    soup = BeautifulSoup(html, "lxml")
    items: List[Dict[str, Any]] = []
    seen = set()

    for a in soup.select('a[href*="/usedcar/detail/"]'):
        href = a.get("href", "")
        if not href:
            continue
        url = href if href.startswith("http") else "https://www.carsensor.net" + href
        if url in seen:
            continue
        seen.add(url)

        title = a.get_text(" ", strip=True)

        # Ë¶™„ÇíÊï∞ÈöéÂ±§„Åü„Å©„Å£„Å¶„ÄÅ„Åæ„Å®„Åæ„Çä„ÅÆ„ÅÇ„Çã„ÉÜ„Ç≠„Çπ„Éà„ÇíÂèñÂæó
        block = a
        for _ in range(4):
            if block and block.parent:
                block = block.parent
            else:
                break
        text = (block.get_text(" ", strip=True) if block else a.get_text(" ", strip=True))

        items.append({
            "title": title,
            "url": url,
            "year": year_from_text(text),
            "mileage": km_to_int(text),
            "price": textnum_to_int(text),
            "site": "carsensor",
        })
    return items

def parse_goonet_list(html: str) -> List[Dict[str, Any]]:
    # ‰Ωø„ÅÜ„Å™„ÇâÂêåÊßò„Å´ÂÆüË£Ö„ÄÇ‰ªäÂõû„ÅØÊú™‰ΩøÁî®„ÄÇ
    soup = BeautifulSoup(html, "lxml")
    out: List[Dict[str, Any]] = []
    for a in soup.select('a[href*="/usedcar/"]'):
        href = a.get("href", "")
        if not href:
            continue
        url = href if href.startswith("http") else "https://www.goo-net.com" + href
        t = a.get_text(" ", strip=True)
        out.append({
            "title": a.get_text(strip=True),
            "url": url,
            "year": year_from_text(t),
            "mileage": km_to_int(t),
            "price": textnum_to_int(t),
            "site": "goonet",
        })
    return out

SITE_PARSERS = {
    "carsensor": parse_carsensor_list,
    "goonet":    parse_goonet_list,
}

# --------- Ë©≥Á¥∞„Éö„Éº„Ç∏Ë£úÂÆå ---------
def enrich_from_detail(it: Dict[str, Any]) -> None:
    """‰∏ÄË¶ß„ÅßÊäú„Åë„Åü year/mileage/price „ÇíË©≥Á¥∞HTML„Åã„ÇâË£úÂÆå"""
    try:
        h = fetch(it["url"])
    except Exception:
        return
    if not it.get("year"):
        it["year"] = year_from_text(h) or it.get("year", 0)
    if not it.get("mileage"):
        it["mileage"] = km_to_int(h) or it.get("mileage", 0)
    if not it.get("price"):
        it["price"] = textnum_to_int(h) or it.get("price", 0)

# --------- „Çø„Ç§„Éà„É´Ê≠£Ë¶èÂåñ & „Ç≠„Éº„ÉØ„Éº„Éâ„Éï„Ç£„É´„ÇøÔºàSUV„Å†„Åë/„Éè„Çπ„É©„ÉºÈô§Â§ñÔºâ ---------
def _norm(s: str) -> str:
    return normalize("NFKC", (s or "")).upper()

def keyword_filter(items, include_keywords=None, exclude_keywords=None):
    inc = [_norm(k) for k in (include_keywords or [])]
    exc = {_norm(k) for k in ((exclude_keywords or []) + ["„Éè„Çπ„É©„Éº", "HUSTLER"])}
    out = []
    for it in items:
        title_n = _norm(it.get("title", ""))
        if inc and not any(k in title_n for k in inc):
            continue
        if any(k in title_n for k in exc):
            continue
        out.append(it)
    return out

# --------- Áõ∏Â†¥/Âà§ÂÆöÔºà„É´„Éº„É´Ôºâ ---------
def _percentile(sorted_list, p: float):
    if not sorted_list:
        return None
    k = (len(sorted_list) - 1) * p
    f, c = math.floor(k), math.ceil(k)
    if f == c:
        return sorted_list[int(k)]
    return sorted_list[f] * (c - k) + sorted_list[c] * (k - f)

def compute_price_stats(items):
    prices = [it.get("price") for it in items if it.get("price")]
    if len(prices) < 4:
        return {"median": None, "q25": None}
    s = sorted(prices)
    return {"median": _percentile(s, 0.5), "q25": _percentile(s, 0.25)}

# --------- ÂàÜ‰ΩçÂõûÂ∏∞ OOF: p50 / p20 ‰∫àÊ∏¨ ---------
def _feat(it: Dict[str, Any]):
    title = (it.get("title") or "")
    return [
        it.get("year") or 0,
        it.get("mileage") or 0,
        1 if "„Çµ„É≥„É´„Éº„Éï" in title else 0,
        1 if "„É¨„Ç∂„Éº" in title else 0,
        1 if "BOSE" in title else 0,
        1 if "JBL"  in title else 0,
    ]

def oof_quantile_preds(collected: List[Dict[str, Any]], alphas=(0.5, 0.2)):
    X = np.array([_feat(it) for it in collected], dtype=float)
    y = np.array([it.get("price") or 0 for it in collected], dtype=float)
    n = len(collected)
    if n < 12 or y.sum() == 0:
        return {a: [None]*n for a in alphas}
    kf = KFold(n_splits=3, shuffle=True, random_state=42)
    out = {a: np.zeros(n) for a in alphas}
    for a in alphas:
        preds = np.zeros(n)
        for tr, te in kf.split(X):
            mdl = GradientBoostingRegressor(loss="quantile", alpha=a, random_state=42)
            mdl.fit(X[tr], y[tr])
            preds[te] = mdl.predict(X[te])
        out[a] = preds
    return {a: v.tolist() for a, v in out.items()}

def assess_deal(it, stats, cfg):
    now = datetime.now().year
    price = it.get("price") or 0
    year  = it.get("year") or 0
    km    = it.get("mileage") or 0
    age   = max(0, now - year) if year else 15

    median = (stats or {}).get("median") or 0
    price_ratio = (price / median) if (price and median) else 1.0

    # „Éô„Éº„ÇπÔºà‰æ°Ê†ºÊØîÈáçË¶ñÔºâ
    if price_ratio <= 0.6:   base = 95
    elif price_ratio <= 0.7: base = 85
    elif price_ratio <= 0.8: base = 75
    elif price_ratio <= 0.9: base = 60
    elif price_ratio <= 1.0: base = 45
    else:                    base = 30

    # ÂæÆË™øÊï¥ÔºàÂπ¥Âºè/Ë∑ùÈõ¢/ÊñáÂ≠óÔºâ
    adj = 0
    if age <= 8: adj += 5
    if km and km <= 0.7 * cfg.get("mileage_max", 150_000): adj += 5
    title = (it.get("title") or "")
    if any(k in title for k in ["„Çµ„É≥„É´„Éº„Éï", "„É¨„Ç∂„Éº", "BOSE", "JBL"]): adj += 3
    if any(k in title for k in ["‰øÆÂæ©Ê≠¥", "‰∫ãÊïÖ", "ÂÜ†Ê∞¥"]):               adj -= 20

    score = max(0, min(100, base + adj))

    # ‰∫àÊ∏¨„Éô„Éº„ÇπÔºàOOFÔºâ„Éñ„Éº„Çπ„Éà
    pred50 = it.get("pred_p50")
    pred20 = it.get("pred_p20")
    gap = None
    if isinstance(pred50, (int, float)) and pred50 > 0:
        gap = pred50 - price
        if gap > 500_000:
            score = min(100, score + 8)
        elif gap > 300_000:
            score = min(100, score + 5)
    if isinstance(pred20, (int, float)) and pred20 > 0 and price <= pred20:
        score = min(100, score + 5)

    # Á∑äÊÄ•Â∫¶Ôºà1-5Ôºâ
    if score >= 90:   urgency = 5
    elif score >= 80: urgency = 4
    elif score >= 70: urgency = 3
    elif score >= 60: urgency = 2
    else:             urgency = 1
    if price_ratio <= 0.6:
        urgency = min(5, urgency + 1)
    if gap is not None and gap > 500_000:
        urgency = min(5, urgency + 1)

    # Áõ∏Â†¥‰∏çÊòé„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºà‰∏≠Â§ÆÂÄ§„ÅåÁÑ°„ÅÑÂõûÔºâ
    if not median:
        if (price and price <= 0.8 * cfg.get("price_max", 9e9)
            and year and year >= cfg.get("year_min", 0) + 2
            and km and km <= 0.7 * cfg.get("mileage_max", 9e9)):
            urgency = max(urgency, 4)
            score   = max(score, 80)

    it["price_ratio"] = round(price_ratio, 2) if median else "-"
    it["score"]       = score
    it["urgency"]     = int(urgency)
    it["deal_gap"]    = int(gap) if gap is not None else None

# --------- DiscordÈÄöÁü•ÔºàÂÖ±ÈÄöÔºâ ---------
def _post_discord(items: List[Dict[str, Any]], webhook_url: str | None, title_prefix: str):
    if not items:
        print(f"[INFO] {title_prefix} ÈÄöÁü•ÂØæË±°„Å™„Åó")
        return
    if DRY_RUN:
        preview = [{
            "title": it.get("title"),
            "url": it.get("url"),
            "price": it.get("price"),
            "year": it.get("year"),
            "mileage": it.get("mileage"),
            "score": it.get("score"),
            "urgency": it.get("urgency"),
            "price_ratio": it.get("price_ratio"),
            "deal_gap": it.get("deal_gap"),
        } for it in items]
        print(f"[DRY-RUN] {title_prefix} payload preview:", json.dumps(preview, ensure_ascii=False, indent=2))
        return
    if not webhook_url:
        print(f"[INFO] {title_prefix} Áî®WebhookÊú™Ë®≠ÂÆö„ÄÇÈÄöÁü•„Çπ„Ç≠„ÉÉ„Éó")
        return

    embeds = []
    for it in items[:5]:  # ÂêÑ„Ç´„ÉÜ„Ç¥„É™ÊúÄÂ§ß5‰ª∂
        price = f"{it.get('price',0):,}ÂÜÜ" if it.get('price') else "‚Äî"
        year  = it.get("year") or "‚Äî"
        km    = it.get("mileage") or 0
        km_s  = f"{km:,}km" if km else "‚Äî"
        gap   = it.get("deal_gap")
        gap_s = (f"+{gap:,}ÂÜÜ" if isinstance(gap, int) and gap is not None and gap >= 0
                 else (f"{gap:,}ÂÜÜ" if gap is not None else "‚Äî"))
        p50 = it.get("pred_p50"); p20 = it.get("pred_p20")
        p50s = f"{int(p50):,}ÂÜÜ" if isinstance(p50, (int,float)) else "‚Äî"
        p20s = f"{int(p20):,}ÂÜÜ" if isinstance(p20, (int,float)) else "‚Äî"
        embeds.append({
            "title": (it.get("title") or "")[:256],
            "url": it.get("url"),
            "description": f"{it.get('site','')} | {year}Âπ¥ | {km_s} | {price}",
            "fields": [
                {"name": "Score",       "value": str(it.get("score", 0)),         "inline": True},
                {"name": "Price Ratio", "value": str(it.get("price_ratio", '-')), "inline": True},
                {"name": "Urgency",     "value": "üî•" * it.get("urgency", 1),     "inline": True},
                {"name": "Deal Gap",    "value": gap_s,                            "inline": True},
                {"name": "p50(pred)",   "value": p50s,                             "inline": True},
                {"name": "p20(pred)",   "value": p20s,                             "inline": True},
            ],
        })
    payload = {
        "content": f"{title_prefix}\n{datetime.now():%Y-%m-%d %H:%M}",
        "embeds": embeds,
    }
    try:
        r = requests.post(webhook_url, json=payload, timeout=12)
        r.raise_for_status()
        print(f"[OK] Discord ÈÄöÁü•ÂÆå‰∫Ü: {title_prefix}")
    except Exception as e:
        print(f"[WARN] Discord ÈÄöÁü•Â§±Êïó({title_prefix}): {e}")

# ÂæåÊñπ‰∫íÊèõ„ÅÆÂçò‰∏ÄÈÄöÁü•ÔºàÂøÖË¶Å„Å™„ÇâÂà©Áî®Ôºâ
def discord_notify(items: List[Dict[str, Any]]):
    url = WEBHOOK_MAIN  # Êóß: DISCORD_WEBHOOK_URL „ÇíÂê´„ÇÄ
    if DRY_RUN:
        preview = [{
            "title": it.get("title"),
            "url": it.get("url"),
            "price": it.get("price"),
            "year": it.get("year"),
            "mileage": it.get("mileage"),
            "score": it.get("score"),
            "urgency": it.get("urgency"),
            "price_ratio": it.get("price_ratio"),
            "deal_gap": it.get("deal_gap"),
        } for it in items if it.get("urgency",1) >= IMMEDIATE_URGENCY_MIN]
        print("[DRY-RUN] (legacy) Discord payload preview:", json.dumps(preview, ensure_ascii=False, indent=2))
        return
    if not url:
        print("[INFO] DISCORD_WEBHOOK_URL Êú™Ë®≠ÂÆö„ÄÇDiscordÈÄöÁü•„Çí„Çπ„Ç≠„ÉÉ„Éó")
        return
    cands = [x for x in items if x.get("urgency", 1) >= IMMEDIATE_URGENCY_MIN][:5]
    if not cands:
        print("[INFO] DiscordÈÄöÁü•ÂØæË±°„Å™„ÅóÔºàÂç≥Ë≤∑„ÅÑË©≤ÂΩì„Å™„ÅóÔºâ")
        return
    _post_discord(cands, url, "üöÄ Âç≥Ë≤∑„ÅÑ„É¨„Éô„É´ÔºàlegacyÔºâ")

# --------- ÂÄôË£ú„ÅÆÂàÜÂâ≤ÔºàÂç≥Ë≤∑„ÅÑ / „ÅÇ„Çä„Åã„ÇÇÔºâ ---------
def split_candidates(all_items: List[Dict[str, Any]]):
    immediate = [x for x in all_items if x.get("urgency",1) >= IMMEDIATE_URGENCY_MIN]
    maybe = [x for x in all_items
             if (x.get("urgency",1) == 3) or
                (MAYBE_SCORE_MIN <= x.get("score",0) <= MAYBE_SCORE_MAX)]
    ids = set(id(x) for x in immediate)
    maybe = [x for x in maybe if id(x) not in ids]
    immediate.sort(key=lambda x: x.get("score",0), reverse=True)
    maybe.sort(key=lambda x: x.get("score",0), reverse=True)
    return immediate[:5], maybe[:5]

# --------- „É°„Ç§„É≥ ---------
def main():
    targets = DEFAULT_TARGETS
    tj = os.getenv("TARGETS_JSON")
    if tj:
        try:
            targets = json.loads(tj)
        except Exception as e:
            print(f"[WARN] TARGETS_JSON „ÅÆË™≠„ÅøËæº„ÅøÂ§±Êïó: {e}")

    all_picks: List[Dict[str, Any]] = []
    all_items: List[Dict[str, Any]] = []

    for cfg in targets:
        site = cfg.get("site")
        url  = cfg.get("url")
        pages = int(cfg.get("pages", 1))
        parser = SITE_PARSERS.get(site)
        if not parser:
            print(f"[SKIP] Êú™ÂØæÂøú„Çµ„Ç§„Éà: {site}")
            continue

        collected: List[Dict[str, Any]] = []
        mobile_base = _ensure_mobile_url(url)

        for page in range(1, pages + 1):
            u = _with_page(mobile_base, page)
            try:
                print(f"[GET] {u}")
                html = fetch(u)
                items = parser(html)
                # SUV„ÅÆ„Åø + „Éè„Çπ„É©„ÉºÈô§Â§ñÔºàË°®Ë®ò„ÇÜ„ÇåÂê∏ÂèéÔºâ
                items = keyword_filter(
                    items,
                    include_keywords=cfg.get("include_keywords"),
                    exclude_keywords=cfg.get("exclude_keywords")
                )

                # Ë∂≥„Çä„Å™„ÅÑÊï∞ÂÄ§„ÅØË©≥Á¥∞1Âõû„ÅßË£úÂÆåÔºàÂèñ„Çä„Åô„ÅéÈò≤Ê≠¢„Åß‰∏ä‰Ωç30‰ª∂„Åæ„ÅßÔºâ
                need_enrich = [x for x in items if (not x.get("price") or not x.get("year") or not x.get("mileage"))]
                for it in need_enrich[:30]:
                    enrich_from_detail(it)

                collected.extend(items)
                print(f"  ‚îî parsed {len(items)} items (page {page})")
            except requests.HTTPError as e:
                code = getattr(e.response, "status_code", "?")
                print(f"[HTTP {code}] {u}")
            except Exception as e:
                print(f"[ERR] {u}: {e}")

        # ÂàÜ‰ΩçÂõûÂ∏∞ OOF ‰∫àÊ∏¨ÔºàÂçÅÂàÜ„Å™‰ª∂Êï∞„Åå„ÅÇ„ÇãÂõû„Å†„ÅëÔºâ
        qpreds = oof_quantile_preds(collected, alphas=(0.5, 0.2))
        for i, it in enumerate(collected):
            it["pred_p50"] = qpreds.get(0.5, [None]*len(collected))[i] if collected else None
            it["pred_p20"] = qpreds.get(0.2, [None]*len(collected))[i] if collected else None

        # ‰∏≠Â§ÆÂÄ§Áõ∏Â†¥ ‚Üí Ë©ï‰æ°
        stats = compute_price_stats(collected)
        for it in collected:
            assess_deal(it, stats, cfg)

        all_items.extend(collected)

        # CSVÁî®„ÅÆ‰∏ä‰ΩçÂÄôË£úÔºàL4+ÂÑ™ÂÖà„ÄÅË∂≥„Çä„Å™„Åë„Çå„Å∞„Çπ„Ç≥„Ç¢„ÅßË£úÂÆåÔºâ
        picks = [x for x in collected if x.get("urgency", 1) >= 4]
        if len(picks) < 8:
            extra = sorted([x for x in collected if x not in picks],
                           key=lambda x: x.get("score", 0), reverse=True)
            picks.extend(extra[:8 - len(picks)])
        print(f"  ‚Üí ÂÄôË£ú {len(picks)} ‰ª∂Ôºà{cfg.get('name')}Ôºâ")
        all_picks.extend(picks)

    # ÂÖ®‰Ωì„Åã„Çâ‰∏ä‰Ωç10„ÇíÊõ∏„ÅçÂá∫„ÅóÔºà‰∫íÊèõÔºâ
    all_picks.sort(key=lambda x: x.get("score", 0), reverse=True)
    top = all_picks[:10]

    out_csv = "results.csv"
    with open(out_csv, "w", newline="", encoding="utf-8-sig") as f:
        w = csv.writer(f)
        w.writerow(["title","url","site","year","mileage","price",
                    "score","price_ratio","urgency","pred_p50","pred_p20","deal_gap"])
        for it in top:
            p50 = it.get("pred_p50"); p20 = it.get("pred_p20")
            w.writerow([
                it.get("title", ""), it.get("url", ""), it.get("site", ""),
                it.get("year", 0), it.get("mileage", 0), it.get("price", 0),
                it.get("score", 0), it.get("price_ratio", "-"), it.get("urgency", 1),
                int(p50) if isinstance(p50, (int,float)) else "",
                int(p20) if isinstance(p20, (int,float)) else "",
                it.get("deal_gap","")
            ])
    print(f"[OK] CSV Âá∫Âäõ: {out_csv}Ôºà{len(top)}‰ª∂Ôºâ")

    # ‰∫åÊÆµÈöéÈÄöÁü•
    immediate, maybe = split_candidates(all_items)
    _post_discord(immediate, WEBHOOK_MAIN,  "üöÄ Âç≥Ë≤∑„ÅÑ„É¨„Éô„É´")
    _post_discord(maybe,    WEBHOOK_MAYBE, "ü§î „ÅÇ„Çä„Åã„ÇÇ„É¨„Éô„É´")

    # ‰∫íÊèõ„ÅÆÂçò‰∏ÄÈÄöÁü•„Çí‰Ωø„ÅÜÂ†¥Âêà„ÅØÂøÖË¶Å„Å´Âøú„Åò„Å¶
    # discord_notify(top)

if __name__ == "__main__":
    main()
